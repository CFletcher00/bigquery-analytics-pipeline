ğŸ“Š BigQuery Analytics Pipeline

A beginner-friendly but professionally structured analytics pipeline that simulates how data flows into BigQuery for downstream analysis.
Built using Python, Pandas, and a mock BigQuery layer, so it works on any machine without needing Google Cloud.

ğŸš€ Project Overview

This project demonstrates the core steps of a data engineering analytics workflow:

Extract CSV data

Transform the data with Python + Pandas

Load into a Mock BigQuery Warehouse (simulating a real DWH)

Display aggregated results

This mirrors the daily workflow of data engineers working with GCP BigQuery, Data Lakes, and analytical pipelines â€” exactly the type of work PayPay, LINE, Yahoo!, and other Japanese fintech companies do.

ğŸ§  Why This Project Matters

It shows you understand:

Data extraction & file-based ingestion

Data cleaning & transformation

Analytical SQL-style aggregation with Python

Warehouse loading patterns

Structuring a real data engineering project

Recruiters will view this as evidence you understand ETL fundamentals.

ğŸ§± Architecture Diagram
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV File   â”‚ â”€â”€â–º â”‚  ETL Script  â”‚ â”€â”€â–º â”‚  BigQuery Mock Storage â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Extract   â†’   Clean   â†’   Aggregate   â†’   Load

ğŸ“ Project Structure
bigquery-analytics-pipeline/
â”‚â”€â”€ data/
â”‚    â””â”€â”€ transactions.csv
â”‚
â”‚â”€â”€ pipeline/
â”‚    â””â”€â”€ bigquery_mock.py
â”‚
â”‚â”€â”€ main.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

ğŸ§ª Example Result

The pipeline outputs a mock BigQuery table:

=== ğŸ“¦ BigQuery Mock Storage ===

Table: customer_totals
   customer_id  amount
0          101   35.98
1          102    5.99
2          103  199.99

ğŸ› ï¸ Technologies Used

Python 3.11+

Pandas

PyArrow

Mock BigQuery API

CSV ingestion

No cloud account needed â€” this simulates the full workflow locally.

âš™ï¸ Setup & Run
1ï¸âƒ£ Install requirements
pip install -r requirements.txt

2ï¸âƒ£ Run the pipeline
python main.py


The result prints instantly to your terminal.

ğŸ§© How It Works
âœ” Extract

Reads incoming CSV data containing transaction logs.

âœ” Transform

Cleans data

Removes bad rows

Aggregates totals per customer

âœ” Load

Loads results into a mock BigQuery object that behaves like a table storage layer.

âœ” Display

Prints final BigQuery-style table.

ğŸŒ± Future Enhancements

These are great additions if you want to expand the project later:

Export aggregated table into Parquet

Add SQL querying layer

Add Airflow orchestration

Integrate actual BigQuery if you get a GCP free trial

Write unit tests for transformations

ğŸ™‹â€â™‚ï¸ Author

Christian Fletcher
GitHub: https://github.com/CFletcher00
